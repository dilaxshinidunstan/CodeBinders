{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3KGkXA8qUrum","executionInfo":{"status":"ok","timestamp":1748681712930,"user_tz":-330,"elapsed":11297,"user":{"displayName":"Sugendran Pranavan","userId":"10144178676014855039"}}},"outputs":[],"source":["\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","import nltk\n","from nltk.corpus import stopwords\n","from textblob import TextBlob\n","from sklearn.preprocessing import MinMaxScaler\n","from prophet import Prophet\n","from nltk.tokenize import TreebankWordTokenizer\n","\n","import pandas as pd\n","import json\n","import re\n","\n","def generate_trending_keywords():\n","    # Optional NLTK downloads\n","    nltk.download('punkt')\n","    nltk.download('stopwords')\n","\n","    # Load data\n","    df_search = pd.read_csv('/content/drive/MyDrive/HackFest_Dataset/search_trends.csv')\n","    df_sales = pd.read_csv('/content/drive/MyDrive/HackFest_Dataset/sales_data.csv')\n","    df_feedback = pd.read_csv('/content/drive/MyDrive/HackFest_Dataset/customer_feedback.csv')\n","    df_catalog = pd.read_csv('/content/drive/MyDrive/HackFest_Dataset/product_catalog.csv')\n","\n","    # Convert dates\n","    df_search['timestamp'] = pd.to_datetime(df_search['timestamp'])\n","    df_sales['timestamp'] = pd.to_datetime(df_sales['timestamp'])\n","    df_feedback['date'] = pd.to_datetime(df_feedback['date'])\n","\n","    # Preprocess search queries\n","    stop_words = set(stopwords.words('english'))\n","    tokenizer = TreebankWordTokenizer()\n","\n","    def clean_and_tokenize(query):\n","        query = re.sub(r'[^a-zA-Z\\s]', '', str(query).lower())\n","        tokens = tokenizer.tokenize(query)\n","        return [word for word in tokens if word not in stop_words]\n","\n","    df_search['tokens'] = df_search['query'].apply(clean_and_tokenize)\n","    df_exploded = df_search.explode('tokens')\n","    df_exploded = df_exploded.rename(columns={'tokens': 'keyword'})\n","\n","    # Aggregate keyword frequency\n","    df_token_time = df_exploded.groupby(['timestamp', 'keyword'])['frequency'].sum().reset_index()\n","\n","    # --- SALES DATA ---\n","    product_category_map = dict(zip(df_catalog['productId'], df_catalog['category']))\n","    df_sales['domain'] = df_sales['productId'].map(product_category_map)\n","\n","    df_sales_agg = df_sales.groupby(['timestamp', 'domain'])['quantitySold'].sum().reset_index()\n","    df_sales_agg.rename(columns={'quantitySold': 'totalSales'}, inplace=True)\n","\n","    # --- CUSTOMER FEEDBACK ---\n","    df_feedback['sentiment'] = df_feedback['commentText'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n","    df_feedback['domain'] = df_feedback['productId'].map(product_category_map)\n","\n","    df_sentiment = df_feedback.groupby(['date', 'domain'])['sentiment'].mean().reset_index()\n","    df_sentiment.rename(columns={'date': 'timestamp', 'sentiment': 'avgSentiment'}, inplace=True)\n","\n","    # --- Map keywords to domain ---\n","    catalog_keywords = set()\n","    for col in ['title', 'category', 'modifiers', 'keywords']:\n","        text = \" \".join(df_catalog[col].dropna().astype(str).str.lower())\n","        catalog_keywords.update(tokenizer.tokenize(text))\n","    catalog_keywords = list(catalog_keywords)\n","\n","    def guess_domain(token):\n","        for domain in df_catalog['category'].unique():\n","            if token in domain.lower():\n","                return domain\n","        return 'unknown'\n","\n","    df_token_time['domain'] = df_token_time['keyword'].apply(guess_domain)\n","\n","    # Merge with sales and sentiment\n","    df_all = df_token_time.merge(df_sales_agg, on=['timestamp', 'domain'], how='left')\n","    df_all = df_all.merge(df_sentiment, on=['timestamp', 'domain'], how='left')\n","    df_all['totalSales'] = df_all['totalSales'].fillna(0)\n","    df_all['avgSentiment'] = df_all['avgSentiment'].fillna(0)\n","\n","    # Normalize features\n","    scaler = MinMaxScaler()\n","    df_all[['frequency_norm', 'sales_norm', 'sentiment_norm']] = scaler.fit_transform(\n","        df_all[['frequency', 'totalSales', 'avgSentiment']]\n","    )\n","\n","    # Compute composite score\n","    w_search, w_sales, w_sentiment = 0.5, 0.3, 0.2\n","    df_all['composite_score'] = (\n","        w_search * df_all['frequency_norm'] +\n","        w_sales * df_all['sales_norm'] +\n","        w_sentiment * df_all['sentiment_norm']\n","    )\n","\n","    # --- Forecasting ---\n","    forecast_dict = {}\n","    top_keywords = df_all.groupby('keyword')['composite_score'].sum().sort_values(ascending=False).head(50).index.tolist()\n","\n","    for keyword in top_keywords:\n","        sub_df = df_all[df_all['keyword'] == keyword][['timestamp', 'composite_score']]\n","        sub_df = sub_df.rename(columns={'timestamp': 'ds', 'composite_score': 'y'})\n","\n","        if len(sub_df) >= 5:\n","            model = Prophet(daily_seasonality=True)\n","            model.fit(sub_df)\n","\n","            future = model.make_future_dataframe(periods=7)\n","            forecast = model.predict(future)\n","\n","            total_future_score = forecast.tail(7)['yhat'].sum()\n","            forecast_dict[keyword] = total_future_score\n","\n","    # Sort and save top 20 trending keywords\n","    sorted_keywords = sorted(forecast_dict.items(), key=lambda x: x[1], reverse=True)\n","    top_keywords_output = [{'keyword': kw, 'forecast_score': round(score, 2)} for kw, score in sorted_keywords[:20]]\n","\n","    with open('/content/drive/MyDrive/HackFest_Dataset/toptrendingkeywords.json', 'w') as f:\n","        json.dump(top_keywords_output, f, indent=4)\n","\n","    print(\"‚úÖ Saved top trending keywords to toptrendingkeywords.json\")\n","\n","# You can call this function to run\n","# generate_trending_keywords()\n","\n","\n","\n","def generate_recommendations():\n","    # 1. Load trending keywords\n","    with open('/content/drive/MyDrive/HackFest_Dataset/toptrendingkeywords.json', 'r') as f:\n","        trending_keywords = json.load(f)\n","\n","    # Convert to DataFrame\n","    df_trending = pd.DataFrame(trending_keywords)\n","\n","    # 2. Load product catalog\n","    df_catalog = pd.read_csv('/content/drive/MyDrive/HackFest_Dataset/product_catalog.csv')\n","\n","    # Fill NaNs with empty strings to avoid issues\n","    df_catalog.fillna('', inplace=True)\n","\n","    # 3. Combine product details for matching\n","    df_catalog['combined_text'] = (\n","        df_catalog['title'].astype(str) + ' ' +\n","        df_catalog['category'].astype(str) + ' ' +\n","        df_catalog['modifiers'].astype(str) + ' ' +\n","        df_catalog['keywords'].astype(str)\n","    ).str.lower()\n","\n","    # 4. Match products to trending keywords\n","    matched_products = []\n","\n","    for _, row in df_trending.iterrows():\n","        keyword = row['keyword'].lower()\n","        score = row['forecast_score']\n","\n","        # Find products containing the keyword\n","        matches = df_catalog[df_catalog['combined_text'].str.contains(rf'\\b{re.escape(keyword)}\\b', regex=True)].copy()\n","        matches['matched_keyword'] = keyword\n","        matches['forecast_score'] = score\n","\n","        matched_products.append(matches)\n","\n","    # 5. Concatenate all matched products\n","    if matched_products:\n","        df_matched_products = pd.concat(matched_products, ignore_index=True)\n","\n","        # Optional: sort by forecast score\n","        df_matched_products = df_matched_products.sort_values(by='forecast_score', ascending=False)\n","\n","        # 6. Save or display the top results\n","        df_matched_products[['productId', 'title', 'category', 'matched_keyword', 'forecast_score']].to_csv('/content/drive/MyDrive/HackFest_Dataset/recommended_products.csv', index=False)\n","\n","        print(\"Recommended products saved to recommended_products.csv\")\n","    else:\n","        print(\"No products matched the trending keywords.\")\n","  # generate_trend()\n","\n"]},{"cell_type":"code","source":["generate_trending_keywords()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"6CFks0bUVFVp","executionInfo":{"status":"error","timestamp":1748681722680,"user_tz":-330,"elapsed":2276,"user":{"displayName":"Sugendran Pranavan","userId":"10144178676014855039"}},"outputId":"070520ed-d73b-4f42-ce0b-af694808edbb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/HackFest_Dataset/search_trends.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7648e5386b1c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_trending_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-4a363ee05211>\u001b[0m in \u001b[0;36mgenerate_trending_keywords\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdf_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HackFest_Dataset/search_trends.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mdf_sales\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HackFest_Dataset/sales_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdf_feedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/HackFest_Dataset/customer_feedback.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/HackFest_Dataset/search_trends.csv'"]}]},{"cell_type":"code","source":["generate_recommendations()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cv7IkEkdY4FY","executionInfo":{"status":"ok","timestamp":1748679438371,"user_tz":-330,"elapsed":478,"user":{"displayName":"Valethanchan Krishnakishore","userId":"05787467343932389593"}},"outputId":"7d52380a-b5ff-4d6e-a8c7-503fa685b825"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Recommended products saved to recommended_products.csv\n"]}]},{"cell_type":"code","source":["!pip install dash plotly"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qz4jRc57dM6Z","executionInfo":{"status":"ok","timestamp":1748680117378,"user_tz":-330,"elapsed":5442,"user":{"displayName":"Valethanchan Krishnakishore","userId":"05787467343932389593"}},"outputId":"5f6082db-1e61-4940-bb2e-dda65695dbc7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dash\n","  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n","Collecting Flask<3.1,>=1.0.4 (from dash)\n","  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n","Collecting Werkzeug<3.1 (from dash)\n","  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.7.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.13.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\n","Collecting retrying (from dash)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.2.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.2.1)\n","Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.4.26)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->dash) (1.17.0)\n","Downloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Installing collected packages: Werkzeug, retrying, Flask, dash\n","  Attempting uninstall: Werkzeug\n","    Found existing installation: Werkzeug 3.1.3\n","    Uninstalling Werkzeug-3.1.3:\n","      Successfully uninstalled Werkzeug-3.1.3\n","  Attempting uninstall: Flask\n","    Found existing installation: Flask 3.1.1\n","    Uninstalling Flask-3.1.1:\n","      Successfully uninstalled Flask-3.1.1\n","Successfully installed Flask-3.0.3 Werkzeug-3.0.6 dash-3.0.4 retrying-1.3.4\n"]}]},{"cell_type":"code","source":["import dash\n","from dash import html, dcc, dash_table\n","import plotly.express as px\n","import pandas as pd\n","import json\n","\n","# Import your functions\n","#import generate_trending_keywords, generate_recommendations\n","\n","def main():\n","    # --- 1. Generate Trending Keywords and Recommendations ---\n","    print(\"üîç Generating trending keywords and recommendations...\")\n","    #generate_trending_keywords()\n","    generate_recommendations()\n","    print(\"‚úÖ Data generation complete!\")\n","\n","    # --- 2. Load Processed Data ---\n","    with open('/content/drive/MyDrive/HackFest_Dataset/toptrendingkeywords.json', 'r') as f:\n","        trending_keywords = json.load(f)\n","    df_trending = pd.DataFrame(trending_keywords)\n","\n","    df_recommendations = pd.read_csv('/content/drive/MyDrive/HackFest_Dataset/recommended_products.csv')\n","\n","    # --- 3. Create Dash App ---\n","    app = dash.Dash(__name__)\n","    app.title = \"AI-Powered Trend & Recommendation Dashboard\"\n","\n","    # --- 4. Visualizations ---\n","    # Top Trending Keywords Bar Chart\n","    trending_fig = px.bar(\n","        df_trending,\n","        x='keyword',\n","        y='forecast_score',\n","        title=\"üî• Top Trending Keywords Forecast\",\n","        color='forecast_score'\n","    )\n","\n","    # Recommended Products Table\n","    recommendation_table = dash_table.DataTable(\n","        columns=[{\"name\": col, \"id\": col} for col in df_recommendations.columns],\n","        data=df_recommendations.to_dict('records'),\n","        page_size=10,\n","        style_table={'overflowX': 'auto'},\n","        style_cell={'textAlign': 'left'}\n","    )\n","\n","    # --- 5. Layout with Tabs ---\n","    app.layout = html.Div([\n","        html.H1(\"üß† AI-Powered Trend & Recommendation Dashboard\", style={'textAlign': 'center'}),\n","        dcc.Tabs([\n","            dcc.Tab(label='Trending Keywords', children=[\n","                html.H2(\"Top 20 Trending Keywords\"),\n","                dcc.Graph(figure=trending_fig),\n","                html.H4(\"üîç Data Preview\"),\n","                dash_table.DataTable(\n","                    columns=[{\"name\": i, \"id\": i} for i in df_trending.columns],\n","                    data=df_trending.to_dict('records'),\n","                    page_size=10,\n","                    style_table={'overflowX': 'auto'},\n","                    style_cell={'textAlign': 'left'}\n","                )\n","            ]),\n","            dcc.Tab(label='Recommended Products', children=[\n","                html.H2(\"üì¶ Recommended Products\"),\n","                recommendation_table\n","            ])\n","        ])\n","    ])\n","\n","    # --- 6. Run App ---\n","    app.run(debug=True)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"-WNl2do9chcq","executionInfo":{"status":"error","timestamp":1748681648830,"user_tz":-330,"elapsed":171,"user":{"displayName":"Valethanchan Krishnakishore","userId":"05787467343932389593"}},"outputId":"71c9fcf3-903f-48df-cfbb-4d1861f6d04f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'dash'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eb40bc20b0f8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdash_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dash'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}